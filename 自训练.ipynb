{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入数据包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhangyh4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "from numpy import * # 要用到delete操作\n",
    "import numpy as np\n",
    "from collections import defaultdict,Counter\n",
    "import math\n",
    "from gensim import corpora,models\n",
    "from svmutil import *\n",
    "from svm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.读取数据集并对数据进行预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1).读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = pd.read_csv('unlabel_data.txt',names=['lines'],sep='\\t',encoding='utf-8')\n",
    "unlabeled_data['label'] = -1\n",
    "labeled_data = pd.read_csv('label_data.txt',names=['label','lines'],sep='\\t',encoding='utf-8')\n",
    "frame = [labeled_data,unlabeled_data]\n",
    "sum_data = pd.concat(frame,axis = 0)\n",
    "sum_data = sum_data.dropna()\n",
    "sum_data = sum_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2).对数据进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉文本中的空格\n",
    "def process(our_data):\n",
    "    m1 = map(lambda s: s.replace(' ', ''), our_data)\n",
    "    return list(m1)\n",
    "\n",
    "\n",
    "# 让文本只保留汉字\n",
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def format_str(content):\n",
    "    content_str = ''\n",
    "    for i in content:\n",
    "        if is_chinese(i):\n",
    "            content_str = content_str + ｉ\n",
    "    return content_str\n",
    "\n",
    "\n",
    "# 对文本进行jieba分词\n",
    "def fenci(datas):\n",
    "    cut_words = map(lambda s: list(jieba.cut(s)), datas)\n",
    "    return list(cut_words)\n",
    "\n",
    "\n",
    "# 去掉文本中的停用词\n",
    "def drop_stopwords(contents, stopwords):\n",
    "    contents_clean = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhangyh4\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.707 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 读取停用词表\n",
    "stopwords = pd.read_csv('stopwords.txt', index_col=False, sep=\"\\n\", quoting=3, names=['stopword'], encoding='utf-8')\n",
    "\n",
    "# 去掉文本中的空格\n",
    "train_data = process(sum_data.lines.values)\n",
    "\n",
    "# 让文本只保留汉字\n",
    "chinese_list = []\n",
    "for line in train_data:\n",
    "    chinese_list.append(format_str(line))\n",
    "\n",
    "# 对预处理好的文本进行分词\n",
    "df_content = pd.DataFrame({'content_S': chinese_list, 'label': sum_data['label']})\n",
    "content_s = fenci(df_content.content_S.values)\n",
    "data_content = pd.DataFrame({'content': content_s})\n",
    "\n",
    "# 去除停用词\n",
    "contents = data_content.content.values.tolist()\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "contents_clean = drop_stopwords(contents, stopwords)\n",
    "\n",
    "# 将处理好的文本做成列表格式\n",
    "df_data = pd.DataFrame({'contents_clean': contents_clean, 'label': sum_data[\"label\"]})\n",
    "word_list = list(df_data.contents_clean.values)\n",
    "\n",
    "# 将文本处理成tfidf可训练的格式\n",
    "words = []\n",
    "for line_index in range(len(word_list)):\n",
    "    words.append(' '.join(word_list[line_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练tfidf向量可以看912new.ipynb\n",
    "class TfIdf():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def GetTFIDF(self,corpus,IdfPath):\n",
    "        # 新的一批文本\n",
    "        \"\"\"\n",
    "        :param corpus: 需要计算的文本\n",
    "        :param IdfPath: idf文件路径\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        IDF = {}\n",
    "        with open(IdfPath,\"r\",encoding=\"utf-8\")as f2:\n",
    "            for eachline in f2:\n",
    "                IDF[eachline.strip().split(\"\\t\")[0]]=[int(eachline.strip().split(\"\\t\")[1]),float(eachline.strip().split(\"\\t\")[2])]\n",
    "        TFIDF = []\n",
    "        for eachline in corpus:\n",
    "            eachlinetfidf = {}\n",
    "            eachlinetfidf_l2 = {}\n",
    "            l2sum = 0\n",
    "            eachlinecounts = Counter(eachline)\n",
    "            for key,value in eachlinecounts.items():\n",
    "                if key in IDF:\n",
    "                    eachlinetfidf[IDF[key][0]] = IDF[key][1] * value\n",
    "                    l2sum += eachlinetfidf[IDF[key][0]] ** 2\n",
    "            l2sqrt = math.sqrt(l2sum)\n",
    "            for key in eachlinetfidf:\n",
    "                eachlinetfidf_l2[key] = eachlinetfidf[key]/l2sqrt\n",
    "            TFIDF.append(eachlinetfidf_l2)\n",
    "        return TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由此可以得到文本的tfidf向量\n",
    "corpus_ = [[token for token in text.split()] for text in words]\n",
    "TF = TfIdf()\n",
    "\n",
    "tfidf_list = []\n",
    "gettfidf = TF.GetTFIDF(corpus=corpus_,IdfPath=\"idf.txt\")\n",
    "for each in gettfidf:\n",
    "    tfidf_list.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有涉及到的数据\n",
    "# 用于初步训练的数据\n",
    "x_train = tfidf_list[:300] # 已标注数据\n",
    "x_test = tfidf_list[300:] # 未标注数据\n",
    "y_train = list(sum_data[:300].label.values) # 已标注数据的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:760:\n",
      "预测概率为0.980000:\n",
      "Running time:0.8278438786177653 Seconds\n",
      "**********************************************************************\n",
      "第1次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:1381:\n",
      "预测概率为0.980000:\n",
      "Running time:2.2959684308713695 Seconds\n",
      "**********************************************************************\n",
      "第2次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:1833:\n",
      "预测概率为0.980000:\n",
      "Running time:10.393089850308076 Seconds\n",
      "**********************************************************************\n",
      "第3次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:6907:\n",
      "预测概率为0.980000:\n",
      "Running time:30.3392316117232 Seconds\n",
      "**********************************************************************\n",
      "第4次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:3357:\n",
      "预测概率为0.970000:\n",
      "Running time:71.59408775095557 Seconds\n",
      "**********************************************************************\n",
      "第5次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:7050:\n",
      "预测概率为0.970000:\n",
      "Running time:136.3600872506059 Seconds\n",
      "**********************************************************************\n",
      "第6次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:7350:\n",
      "预测概率为0.970000:\n",
      "Running time:243.0871277369485 Seconds\n",
      "**********************************************************************\n",
      "第7次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:7529:\n",
      "预测概率为0.970000:\n",
      "Running time:381.5935037821452 Seconds\n",
      "**********************************************************************\n",
      "第8次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:7774:\n",
      "预测概率为0.970000:\n",
      "Running time:568.2288464892417 Seconds\n",
      "**********************************************************************\n",
      "第9次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:7835:\n",
      "预测概率为0.970000:\n",
      "Running time:801.6418912807756 Seconds\n",
      "**********************************************************************\n",
      "第10次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:7918:\n",
      "预测概率为0.970000:\n",
      "Running time:1079.9254967062593 Seconds\n",
      "**********************************************************************\n",
      "第11次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:8040:\n",
      "预测概率为0.970000:\n",
      "Running time:1340.3172728827203 Seconds\n",
      "**********************************************************************\n",
      "第12次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:8211:\n",
      "预测概率为0.970000:\n",
      "Running time:1685.9423253153382 Seconds\n",
      "**********************************************************************\n",
      "第13次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:8314:\n",
      "预测概率为0.970000:\n",
      "Running time:2124.8741238334696 Seconds\n",
      "**********************************************************************\n",
      "第14次迭代\n",
      "Accuracy = 0.01% (1/10000) (classification)\n",
      "满足预测概率>p的数据量:8462:\n",
      "预测概率为0.970000:\n",
      "Running time:3264.1266809581466 Seconds\n",
      "**********************************************************************\n",
      "第15次迭代\n"
     ]
    }
   ],
   "source": [
    "# 本质思想是不断地把满足条件的未标注数据加入到已标注数据中，训练模型。然后再拿更新的模型去预测未标注的数据，如此循环往复，让模型越来越强大,\n",
    "# 直到最终可以让所有未标注数据满足条件\n",
    "# 未标注的数据从头到尾都不减少，只是从里面取出来而已\n",
    "\n",
    "j = 0    #  循环迭代次数\n",
    "p = 0.98 # 预测概率\n",
    "\n",
    "# 初始化样本集为空，这里的样本集主要是用来添加到训练数据中的，起的是中间过度作用\n",
    "x_train_tmp = []\n",
    "y_train_tmp = []\n",
    "\n",
    "# 预测概率>p的样本集\n",
    "x_train_tmp1 = []\n",
    "y_train_tmp1 = []\n",
    "\n",
    "while len(y_train_tmp) < len(x_test):\n",
    "    print(\"第%d次迭代\" % j)\n",
    "    start = time.clock()\n",
    "    mdhms = time.strftime('%d%H%M%S', time.localtime(time.time()))\n",
    "\n",
    "    x_train.extend(x_train_tmp)\n",
    "    y_train.extend(y_train_tmp)\n",
    "\n",
    "    prob = svm_problem(y_train, x_train)  # 用来训练的数据\n",
    "    param = svm_parameter('-t 2 -c 1 -b 1')  # 训练参数\n",
    "    model = svm_train(prob, param)  # 训练模型\n",
    "    svm_save_model('svm.model' + '_' + mdhms, model)  # 保存模型\n",
    "    p_label, p_acc, p_val = svm_predict(list(range(len(x_test))), x_test, model, options='-b 1')  # 可以得到预测标签以及预测的类别概率\n",
    "\n",
    "    # 做dataframe合并标签，tfidf值，以及分类概率\n",
    "    y_test = pd.DataFrame(p_label, columns=['label'])  # y_test，预测标签。全程用不到，仅仅是为了保存下来，方便后续操作\n",
    "    y_probability = pd.DataFrame(p_val)                # 预测概率\n",
    "    ser = pd.Series(list(x_test))                      # 未标注数据\n",
    "    unlabel_ser = pd.DataFrame(ser, columns=['tfidf'])\n",
    "    # 合并便于处理\n",
    "    df = pd.concat([unlabel_ser, y_probability, y_test], axis=1)\n",
    "\n",
    "    for i in range(len(df[0])):\n",
    "        if df[0][i] >= p or df[1][i] >= p:\n",
    "            x_train_tmp1.append(df.tfidf.values[i])\n",
    "            y_train_tmp1.append(df.label.values[i])\n",
    "\n",
    "    # 如果本次预测概率>p的未标注样本集比上一次的小而且上一次的不可以是空，则让预测概率降低\n",
    "    if len(y_train_tmp1) <= len(y_train_tmp) and len(y_train_tmp) != 0:\n",
    "        p = p - 0.01\n",
    "    else:\n",
    "        y_train_tmp = y_train_tmp1\n",
    "        x_train_tmp = x_train_tmp1\n",
    "    \n",
    "    print(\"满足预测概率>p的数据量:%d\" % len(x_train_tmp1))\n",
    "    \n",
    "    # 要重置样本集为空，下次循环时要用\n",
    "    # 如果不初始化为空会导致x_train重复加入数据\n",
    "    # x_train_tmp（重点在这里）\n",
    "    x_train_tmp1 = []\n",
    "    y_train_tmp1 = []\n",
    "    \n",
    "    j += 1\n",
    "    end = time.clock()\n",
    "    print(\"预测概率为:%f\" % p)\n",
    "    print(\"Running time:%s Seconds\" % (end - start))\n",
    "    print(\"*\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
